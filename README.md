# Deep Learning
Prüfungsaufgabe 1 - Erster Teil

Um dieses Projekt zu starten, folgen Sie dem Binder Badge:      [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/FelixWuensch/Deep-Learning/HEAD)

# Ausführung der Übungsaufgabe

!!! Alle Befehle werden ausgeführt in dem die Zeile ausgewählt wird und anschließend mit Shift (Großschreibtaste) und Enter gestartet wird!!!

1. Im ersten Schritt müssen alle benötigten Libraries installiert und importiert werden, so dass im Folgenden alle Befehle erfolgreich ausgeführt werden können.
2. Danach werden unsere Daten im CSV Format eingelesen und der Index gesetzt. Dies sind jedoch nur die Beschreibungsdaten zum Richtigen Datensatz.
3. Daraufhin wir die „Description“ der Spalte „revol_util“ ausgegeben. Ebenso wird eine Funktion gebaut, welche die „Description“ der ausgewählten Spalte ausgiebt.
4. Nun werden die richtigen Daten geladen und weitere Libraries installiert. Dazu werden auch gleich die Informationen, zum Datensatz, mit .info(), angezeigt.
5. Al nächstes beginnt die explorative Datenanalyse. Dazu wird als erstes ein Countplot der Variable „loan_status“ ausgegeben. Danach ein Histogramm der Spalte „loan_amnt“ um die Verteilung der Gehälter zu visualisieren. 
6. Danach werden die Korrelationswerte des Datensatzes ausgegeben und im zweiten Schritt dann in einer Heatmap visualisiert. Da bei der Visualisierung der Korrelationswerte auffällt, dass die Werte „installment“ und „loan_amnt“ sehr stark korrelieren. Deshalb benutzen wir die zu beginn gebaute Funktion und lassen uns die Beschreibung ausgeben, um nachvollziehen zu können, weshalb die Werte so stark korrelieren. Danach werden die beiden Werte in einem Scatterplot visualisiert.
7. Im nächsten Schritt wird ein Boxplot erstellt mit den Variablen: „loan_status“ und „loan_amnt“. Danach wird die Zusammengefasste Statistik für die Höhe des Kredites gruppiert, nach der Variable „loan_status“, und ausgegeben. 
8. Nun wird begutachtet, welche Einstufungen es für die Darlehen gibt, sowie die du unter Kategorien. Danach wird ein Countplot erstellt welche die Einstufung und den „loan_status“ visualisiert.
9. Als nächstes wird ein Countplot für die Unterkategorien erstellt, um zu erkennen wie viele Kredite mit welcher Kategorie bewertet wurden. Im Anschluss wird noch in Countplot erstellt, der wieder die Verteilung nach dem „loan_status“ visualisiert. Da in den unteren Kategorien die Kredite nicht oft zurückgezahlt werden, wird der Teil noch einmal genauer betrachtet. Dies geschieht wieder mit einem Countplot anhand der Kategorie und der Variable „loan_status“.
10. Der nächste Schritt erstellt eine neu Spalte, welche eine 1 für eine Vollständig zurückgezahlten Kredit enthält und eine null für einen nicht vollständig gezahlten Kredit. Daraufhin wird ein Barplot erstellt, welcher die Korrelationen aller Spalten zur neu eingefügten Spalte aufzeigt.
11. Nun beginnt das Data Preprocessing. Dafür lassen wir uns noch einmal den Kopf des Datensatzes anzeigen mit der .head() Funktion. Ebenso lassen wir uns die länge des Datensatzes, sowie die Summe aller NULL-Werte, für jede Spalte, ausgeben. Als nächstes werden die NULL-Werte als Prozentsatz berechnet und ausgegeben.
12. Als nächstes geht es um die Arbeitsplatzbezeichnung. Hier wird begutachtet wie viele einzigartige Werte es gibt, dann wie oft welcher Beruf vorkommt. Danach wird beschlossen die Arbeitsplatzbezeichnung aus der Tabelle zu entfernen. Dann wird die Spalte „empl_lenght“ ausgegeben, welche Werte es da alles gibt. Diese Werden im nächsten Schritt in eine neue Reihenfolge gebracht. Zum Schluss wird dann noch visualisiert, wie häufig jede Beschäftigungsdauer vorkommt. Ebenso wird in einem Countplot visualisiert wie viele Kredite zurück und nicht zurückgezahlt wurden.
13. Im nächsten Schritt wird untersucht, wie der Zusammenhang zwischen der Anstellungsdauer und der Rückzahlung aussieht. Dazu wird der Prozentsatz der Rückzahlungen pro Kategorie errechnet, ausgegeben und in einem Barplot visualisiert. Da die Spalte der Beschäftigungsdauer jedoch für zu viel Verwirrung sorgt, wird diese entfernt. Daraufhin wird noch einmal die Summe aller NULL-Werte pro Spalte ausgegeben.
14. Nun wird noch einmal der Kopf der Spalte „purpose“ und „title“ ausgegeben. Dann wird die Spalte „title“ entfernt. Dann wir die Spalten Beschreibung von „mort_acc“ ausgegeben und diese zusammengezählt und als eine neue Spalte in den Datensatz eingefügt. Daraufhin werden die Korrelationen zu der neuen Spalte untersucht. Dabei fällt auf, dass „total_acc“ stark korreliert. Dann wird der Mean von „mort_acc“ pro „total_acc“ berechnet. Dann wird eine Funktion gebaut, welche die fehelenden Werte in „mort_acc“ mit dem durchschnittlichen Wert von „mort_acc“ pro „total_acc“ befüllt. Am ende werten noch einmal die Summe aller NULL-Werte ausgegeben.
15. Nun werden alle restlichen NULL-Werte mit der Funktion .dropna() entfernt und das ganze überprüft.
16. Nun werden die kategorischen Variablen behandelt. Dafür werden alles Spalten vom Typ „object“ ausgegeben. Dann wird das Feature „term“ in einen integer Wert umgewandelt. Das Feature „grade“ wird ganz entfernt. Dann werden die Dummyvariablen erstellt. Zunächst für die Spalte „sub_grade“, sowie „verification_status“, „application_type“, „initial_list_status“, sowie „purpose“.
17. Dann werden in der Spalte „home_ownereship“, die Werte „NONE“ und „ANY“ mit „OTHER“ ersetzt. Es wird eine Spalte „zip_code“ hinzugefügt, was aus der Spalte „address“ extrahierbar ist. Die spalte „issue_d“ wird entfernt. Das Feature „earliest_cr_line“ wird bearbeitet. Dabei wird nur die Jahreszahlt entnommen und in ein Zahlenformat gebracht.
18. Nun beginnt die Aufteilung in die Test- und Trainingsdaten. Hierfür wird wieder die Funktion von Sklearn importiert. Dann wird die Spalte „loan_status“ entfernt da es ein Duplikat ist. Dann wird Das Feature und das Label erstellt. Dann erfolgt die Aufteilung in Test- und Trainingsdaten.
19. Bevor das Modell erzeugt wird, werden die Daten noch normalisiert Dann wird das Modell erzeugt. Dazu werden die nötigen Libraries importiert. Dann wird das Modell erstellt. Es erhält einen Input-Layer, zwei Hidden-Layer und eine Output-Layer. Alle Layer erhalten die Relu Aktivation. Dann wird das Modell trainiert, für 25 Epochen. Dann wird das Modell gespeichert, damit nicht immer das ganze Modell ausgeführt werden muss.
20. Nun beginnt die Evaluation des Modells. Dafür wird ein Liniendiagramm erstellt, was die Trainingsperformance anzeigt. Dann wird die Libraries für den Calssification Report und die Confusion Matrix importiert. Der Calssification Report wird ausgegeben und hat eine accuracy im f1-Score von 0.89 Punkten, was schon mal gut ist. Dann wird noch die Confusion Matrix ausgegeben. Diese enthält die Werte [6811, 8847],[70, 63316]. 
21. Dann wird ein zufälliger Kunde ausgewählt mit dem Namen: 305323. Dann wird dieser Kunde dem Model hinzugefügt und predicted ob dieser Kunde einen Kredit erhält. Das Ergebnis ist 1, dass bedeutet der Kunde erhält einen Kredit. Zu guter Letzt wird nachgeschaut ob der Kunde auch den Kredit wieder zurückzahlen kann und auch diese Antwort ist eine 1, was bedeutet er zahlt den kredit zurück. 
Damit ist das Kapitel des Deep Learnings abgeschlossen. Wir haben uns dafür die Daten genauer angeschaut mit der Explorativen Datenanalyse, die Daten bearbeitet und viele Dummyvariablen eingefügt. Dann haben wir die Aufteilung vorgenommen, das Model erstellt, trainiert und eine Vorhersage treffen lassen. Am Ende haben wir einen zufälligen Kunden ausgewählt und geschaut, ob dieser einen Kredit bekommen würde und auch zurückzahlen würde. Damit ist die erste Prüfungsaufgabe abgeschlossen.
